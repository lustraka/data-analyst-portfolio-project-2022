{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1NA6qC2mX0p"
      },
      "source": [
        "# Oil & Gas Industry Case Study\n",
        "Build a business knowledge database using web resources. Keep it short and simple.\n",
        "\n",
        "## Web Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BXvN0ikXmX0v"
      },
      "outputs": [],
      "source": [
        "# Import dependencies\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import date\n",
        "\n",
        "# Define an auxiliary functions\n",
        "def get_url(link):\n",
        "  \"\"\"Extract URL from an <a> element in a google search.\"\"\"\n",
        "  if link[:7] == '/url?q=':\n",
        "    url = re.search(r'q=(.*?)&', link).group(1)\n",
        "  else:\n",
        "    url = re.search(r'\\A(.*?)&', link).group(1)\n",
        "  return url\n",
        "\n",
        "from collections import Counter\n",
        "def count_elements(soup):\n",
        "    \"\"\"Count elements in the soup document\"\"\"\n",
        "    return Counter(tag.name for tag in soup.find_all(True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VLGd6XoHmX0x"
      },
      "outputs": [],
      "source": [
        "# Define core code !Beware: Search returns results when run in Colab!\n",
        "def search_web(search, count=40, debug=False):\n",
        "  \"\"\"Search the web\"\"\"\n",
        "\n",
        "  data = []\n",
        "  \n",
        "  for i in range(0, count, 10):\n",
        "    gsearch = 'https://www.google.com/search?q=' + search + '&start=' + str(i)\n",
        "    if debug:\n",
        "      print(f\"gsearch = {gsearch}\")\n",
        "    gpage = requests.get(gsearch)\n",
        "    if debug:\n",
        "      print(f\"gpage.status_code = {gpage.status_code}\")\n",
        "    gsoup = BeautifulSoup(gpage.content, 'html.parser')\n",
        "    h3_list = gsoup.find_all('h3')\n",
        "    if debug:\n",
        "      print(f\"len(h3_list) = {len(h3_list)}\")\n",
        "\n",
        "    # Continue only if there are some results\n",
        "    if len(h3_list) == 0:\n",
        "      break\n",
        "\n",
        "    for h3 in h3_list:\n",
        "      title = h3.text\n",
        "      if debug:\n",
        "        print(f\"\\th3.text = {h3.text} | h3.parent.attrs.keys() = {h3.parent.attrs.keys()}\")\n",
        "\n",
        "      # Skip the rest of loop if no url in h3.parent\n",
        "      if 'href' not in h3.parent.attrs.keys():\n",
        "        continue\n",
        "\n",
        "      url = get_url(h3.parent['href'])\n",
        "      inn = re.search('//(.*?)/', url).group(1)\n",
        "\n",
        "      data.append([inn, title, url])\n",
        "\n",
        "    print(gpage.status_code, gsearch, len(h3_list))\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc1jDkk0mX0z",
        "outputId": "51b07b8d-d23f-40b9-b114-4eda11ef3352"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200 https://www.google.com/search?q=oil+gas+industry&start=0 11\n",
            "200 https://www.google.com/search?q=oil+gas+industry&start=10 10\n",
            "200 https://www.google.com/search?q=oil+gas+industry&start=20 10\n",
            "200 https://www.google.com/search?q=oil+gas+industry&start=30 10\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['www.eia.gov',\n",
              " 'Where our natural gas comes from - US Energy Information ...',\n",
              " 'https://www.eia.gov/energyexplained/natural-gas/where-our-natural-gas-comes-from.php']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Search the web resources\n",
        "search = 'oil+gas+industry'\n",
        "data = search_web(search)\n",
        "data[-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "znJBCgg9mX01"
      },
      "outputs": [],
      "source": [
        "search_name = search.replace('+','_')\n",
        "# Save results as a CSV file\n",
        "pd.DataFrame(data, columns=['inn','title','url']).to_csv(search_name+'.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WRnLATCImX03"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Save data as a markdown document\n",
        "def save_md(data, search, search_name):\n",
        "  \"\"\"Save data as a markdown document\"\"\"\n",
        "  md_lines = []\n",
        "  md_lines.append(f\"# Searched terms: `{search}`\")\n",
        "  for row in data:\n",
        "    md_lines.append(f\"- **`{row[0]}`** : [{row[1]}]({row[2]})\")\n",
        "  Path(search_name+'.md').write_text('\\n'.join(md_lines))\n",
        "  return\n",
        "\n",
        "save_md(data, search, search_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDh0r-uOwLxF"
      },
      "source": [
        "## Keep a Journal\n",
        "**2022-01-01 Sat**: (1) **Start Prototyping**. An initial version of this notebook has been derived from [20211202_Search_Web_Resources.ipynb](https://github.com/lustraka/data-analyst-portfolio-project-2022/blob/main/cs01_cds_methods/20211202_Search_Web_Resources.ipynb). The code is simplified to just scraping the web and saving the results as a CSV (`['inn','title','url']`) and also an MD file. I realized, that this code returns expected results only when run in Colab environment. Stored files shall be downloaded to the local clone of repository or uploaded to GitHub. (2) **Inspiration**. The building blocks should have the right granularity. Web search should be as simple as possible. Working with data as categorization can be done in subsequent steps of the pipeline. [3 fsp]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "S-Cj9D9AruuO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "20220101-blueprint-prototype.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ca367ec2123462f39d29b1cb00fbec81c42b60930943c35eba205ccbd8e96ce2"
    },
    "kernelspec": {
      "display_name": "Python 3.9.9 64-bit (windows store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
