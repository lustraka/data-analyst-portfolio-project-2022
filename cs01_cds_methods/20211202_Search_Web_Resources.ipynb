{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "name": "20211202_Search_Web_Resources.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lustraka/data-analyst-portfolio-project-2022/blob/main/cs01_cds_methods/20211202_Search_Web_Resources.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mruFBLAQSrq"
      },
      "source": [
        "# Search Web Resources\n",
        "## Define\n",
        "### Outline requirments\n",
        "In order to *search web resources*,\n",
        "- design the structure of a web search database for storing and comparing automated web searches,\n",
        "- design and test functions for\n",
        "  - searching,\n",
        "  - comparing,\n",
        "  - reporting web resources,\n",
        "- design and test an ontology for discovering (minining) and storing domain knowledge,\n",
        "- populate system in order to compile 2021 annual report on CDS and their methods.\n",
        "\n",
        "### Reflext the previous code\n",
        "\n",
        "- Data_Analysis_Workouts > [20211111_Scrape_WebPages_Root.ipynb](https://github.com/lustraka/Data_Analysis_Workouts/blob/main/Wrangle_Data/Scrape_Google_Search/20211111_Scrape_WebPages_Root.ipynb)\n",
        "  - gather results of the google search (`goo_urls`)\n",
        "  - iterate over `goo_urls` to gather web resources (including the text of a web page if `status_code == 200`)\n",
        "  - create a dataframe with `columns=['id' ,'title', 'wp_url', 'status', 'status_ts', 'text', 'text_len'])`\n",
        "  - store dataframe to SQLite database (`SQLAlchemy`)\n",
        "- suitecrm > [20211126-WebScrape-Pytude.ipynb](https://github.com/lustraka/suitecrm/blob/main/iim/20211126-WebScrape-Pytude.ipynb) >> [Scrape_Google_Sandbox.md](https://github.com/lustraka/suitecrm/blob/main/iim/Scrape_Google_Sandbox.md)\n",
        "  - define `scrape_web(search, count)` function to return a dataframe with `columns=['url', 'site', 'title', 'rec_created']`\n",
        "  ```python\n",
        "  search = 'https://www.google.com/search?q=extern√≠+hodnotitel'\n",
        "df = scrape_web(search, 100)\n",
        "  ```\n",
        "  - export search results to markdown\n",
        "\n",
        "### Design an initial data structure of `url_master.csv`\n",
        "\n",
        "Variable | Description\n",
        "- | -\n",
        "term | An unique idenitifier curated by hand.\n",
        "title | A title of the web page retrieved from the google search results.\n",
        "url | A URL of the web page retrieved from the google search.\n",
        "inn | An Internet node (hostname).\n",
        "accessed | A date of the web page access.\n",
        "search | The posted search string.\n",
        "\n",
        "### Define the `search_web(master, search, count)` function\n",
        "Use the Google search engine to find `count` responses to `search`.\n",
        "\n",
        "Params:\n",
        "- `master`: a master dataframe for identifying duplicates\n",
        "- `search`: search terms connected with `+`\n",
        "- `count`: required number of results, default is `40`; pages are retrieved in multiples of 10\n",
        "\n",
        "Returns:\n",
        "- a list with records with same columns as in `url_master`\n",
        "\n",
        "If a web resource is already in `url_master`, then `term` is filled with that value, otherwise it has the `tbd` value."
      ],
      "id": "4mruFBLAQSrq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytgpotRkd-wd"
      },
      "source": [
        "# Import dependencies\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import date\n",
        "import os\n",
        "\n",
        "# Define an auxiliary function\n",
        "def get_url(link):\n",
        "  \"\"\"Extract URL from a google search <a> element\"\"\"\n",
        "  if link[:7] == '/url?q=':\n",
        "    url = re.search(r'q=(.*?)&', link).group(1)\n",
        "  else:\n",
        "    url = re.search(r'\\A(.*?)&', link).group(1)\n",
        "  return url\n",
        "\n",
        "# Define load_url_master function\n",
        "def load_url_master(name='url_master.csv'):\n",
        "  \"\"\"Load master file or initialize empty dataframe\"\"\"\n",
        "\n",
        "  if os.path.isfile(name):\n",
        "    df = pd.read_csv(name)\n",
        "  else:\n",
        "    df = pd.DataFrame(columns=['term', 'title', 'url', 'inn', 'accessed', 'search'])\n",
        "  \n",
        "  return df\n"
      ],
      "id": "ytgpotRkd-wd",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMB948THgBp8"
      },
      "source": [
        "# Define search_web() function\n",
        "def search_web(master, search, count=40, debug=False):\n",
        "  \"\"\"<fixme> from markdown cell\"\"\"\n",
        "\n",
        "  data = []\n",
        "  accessed = str(date.today())\n",
        "\n",
        "  for i in range(0, count, 10):\n",
        "    gsearch = 'https://www.google.com/search?q=' + search + '&start=' + str(i)\n",
        "    if debug:\n",
        "      print(f\"gsearch = {gsearch}\")\n",
        "    gpage = requests.get(gsearch)\n",
        "    if debug:\n",
        "      print(f\"gpage.status_code = {gpage.status_code}\")\n",
        "    gsoup = BeautifulSoup(gpage.content, 'html.parser')\n",
        "    h3_list = gsoup.find_all('h3')\n",
        "    if debug:\n",
        "      print(f\"len(h3_list) = {len(h3_list)}\")\n",
        "\n",
        "    # Continue only if there are some results\n",
        "    if len(h3_list) == 0:\n",
        "      break\n",
        "\n",
        "    for h3 in h3_list:\n",
        "      title = h3.text\n",
        "      if debug:\n",
        "        print(f\"\\th3.text = {h3.text} | h3.parent.attrs.keys() = {h3.parent.attrs.keys()}\")\n",
        "\n",
        "      # Skip the rest of loop if no url in h3.parent\n",
        "      if 'href' not in h3.parent.attrs.keys():\n",
        "        continue\n",
        "\n",
        "      url = get_url(h3.parent['href'])\n",
        "      inn = re.search('//(.*?)/', url).group(1)\n",
        "      \n",
        "      # Check the url in url_master\n",
        "      check = master.loc[master.url == url]\n",
        "\n",
        "      if check.shape[0] == 0:\n",
        "        term = 'tbd'\n",
        "      elif check.shape[0] == 1:\n",
        "        term = check.term.values[0]\n",
        "      else:\n",
        "        print(f\"Duplicted records for `{check.term.values[0]}` in `url_master`!!\")\n",
        "      \n",
        "      data.append([term, title, url, inn, accessed, search])\n",
        "\n",
        "    print(gpage.status_code, gsearch, len(h3_list))\n",
        "\n",
        "  return data"
      ],
      "id": "gMB948THgBp8",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx2LKXVDkqyK",
        "outputId": "a6aa63c5-ccec-4ae4-e432-329046d17de2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Test web search\n",
        "master = load_url_master()\n",
        "search = 'kitchin+data+revolution'\n",
        "data = search_web(master, search, 11, debug=False)\n",
        "data[-1:]"
      ],
      "id": "bx2LKXVDkqyK",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 https://www.google.com/search?q=kitchin+data+revolution&start=0 12\n",
            "200 https://www.google.com/search?q=kitchin+data+revolution&start=10 10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['tbd',\n",
              "  'Academic books | Rob Kitchin',\n",
              "  'https://www.kitchin.org/%3Fpage_id%3D2',\n",
              "  'www.kitchin.org',\n",
              "  '2021-12-02',\n",
              "  'kitchin+data+revolution']]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOaCuuAGk0_x",
        "outputId": "d65f59f6-8fcc-4378-aeba-a9c385b02d22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for row in data:\n",
        "  print(row[3], row[1])"
      ],
      "id": "lOaCuuAGk0_x",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uk.sagepub.com The Data Revolution | SAGE Publications Ltd\n",
            "methods.sagepub.com Big Data, Open Data, Data Infrastructures & Their Consequences\n",
            "www.amazon.com The Data Revolution: Big Data, Open Data, Data Infrastructures and ...\n",
            "www.theoryculturesociety.org Review: Rob Kitchin, 'The Data Revolution' - Theory, Culture & Society\n",
            "books.google.com The Data Revolution: Big Data, Open Data, Data ... - Google Books\n",
            "www.youtube.com Rob Kitchin talks about big data, open data and the 'data revolution'\n",
            "www.researchgate.net The Data Revolution: Big Data, Open Data, Data Infrastructures and ...\n",
            "thedatarevolutionbook.wordpress.com The Data Revolution | A book about big data, open data, data ...\n",
            "arthistory2015.doingdh.org [PDF] data revolution - Building a Digital Portfolio\n",
            "www.barnesandnoble.com A Critical Analysis of Big Data, Open Data and Data Infrastructures\n",
            "books.google.com The Data Revolution - Rob Kitchin - Google Books\n",
            "onlinelibrary.wiley.com The Data Revolution - Wiley Online Library\n",
            "onlinelibrary.wiley.com Sage Publications. 222+xvii. ISBN: 978‚Äê1446287484, $100.\n",
            "www.powells.com The Data Revolution - Hardcover: 9781529733761 - Powell's Books\n",
            "www.goodreads.com The Data Revolution: Big Data, Open Data, Data Infrastructure...\n",
            "ojs.library.queensu.ca View of Review of Kitchin's 'The Data Revolution'\n",
            "www.thriftbooks.com List of books by author Rob Kitchin - ThriftBooks\n",
            "irishtechnews.ie The Data Revolution by Rob Kitchin, review and interview\n",
            "eprints.whiterose.ac.uk [PDF] Book Review: The Data Revolution: Big Data, Open Data, Data ...\n",
            "www.kitchin.org Academic books | Rob Kitchin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaQyGfIOyLI1"
      },
      "source": [
        "## Plan the next step\n",
        "- save data to `url_review.csv` for by hand review and for the Colab > local > GitHub > Colab cycle\n",
        "- print data for `url_review.md` document which can be used to access pages during analysis\n",
        "- load curated data and append them to `url_master`"
      ],
      "id": "yaQyGfIOyLI1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quf43tuLs6s-"
      },
      "source": [
        ""
      ],
      "id": "quf43tuLs6s-",
      "execution_count": 4,
      "outputs": []
    }
  ]
}